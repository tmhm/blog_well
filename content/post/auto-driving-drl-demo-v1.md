+++
categories = ["Machine Learning"]
title = "基于DRL和TORCS的自动驾驶仿真系统——之策略学习"
date = "2017-07-19T22:14:15+08:00"
description = "接上一篇自动驾驶仿真配置的软件部分，以及demo记录。"
tags = ["自动驾驶","torcs"]

+++


## Robot的软件连接#### 配置说明
TORCS安装PATCH之后，即成为一个基于TORCS的服务器端，补丁程序在服务器的配置选项里面添加了10个被控车辆，如下图1所示（在TORCS中的设置中称之为driver，可以视之为ROBOT执行驾驶行为的载体），控制程序可以通过UDP实现该driver的控制决策。![]()

图2对于如何通过该UDP通信机制进行控制指令的发送以及车辆状态信息的更新原理进行了简要说明。从SCRC的竞赛manual【参考1】中，我们可以知道：在每个20ms的控制周期，TORCS服务器会基于Client发送过来的控制指令与当前车辆状态，根据车辆的动力学模型和道路的物理模型，计算下一状态信息，更新车辆的行驶状态。当服务器在有限时间内没有接收到控制信号的时候，会给出提示，并基于上一次的控制指令继续执行。关于具体的通信代码，本实验借用了一段开源代码snakeoil.py【参考2】。在snakeoil.py代码中，构建了一个Client类，在该类里面实现了get_servers_input()和respond_to_server()函数分别用来获取TORCS服务器端的状态信息和回复控制指令。一个最简单的示例代码如下表1所示。在获取到状态信息后，对状态信息进行分析，然后给出控制指令，填充到一个给定格式的控制数据结构中，再发送回服务器，即可以控制Robot的驾驶行为。代码源文件可以在【参考2】处链接下载；其中Main函数的参数设置路径为：Run->Edit Configurations->Script parameters, 如下图3所示。表1. Robot连接控制代码示例名称描述Main函数输入参数：`--stage 0  --track oralM --steps 8000 --port 3001 --host localhost`执行过程：

```#!/usr/bin/python                              import snakeoil                                if __name__ == "__main__":  C= snakeoil.Client()                       for step in xrange(C.maxSteps,0,-1):   C.get_servers_input()              snakeoil.drive_example(C)              C.respond_to_server()                 C.shutdown()  ```注【1】：状态信息：snakeoil以ServerState类表示，在我们的RL任务中以一个TorcsTask模块体现；控制数据结构：snakeoil代码中通过一个DriverAction类表示，我们在RL任务中以一个TorcsAgent体现。以上两个结构体均可以在snakeoil.Client的__init__()函数处看到，并且在该函数中通过调用子函数setup_connection()，建立起TORCS服务器与控制程序（Client）的链接；即上一篇环境配置中提到的接口程序，就是通过setup_connection()程序启动本机的UDP通信，将TorcsTask和TorcsAgent绑定在一些，形成一个强化学习环境。代码如下图4所示。上一步设置的main参数在此函数中被子函数parse_the_command_line()解析。我们实验project中的snakeoil.py代码是基于【参考2】处的开源代码重写过后的。测试连接状态按照前文所描述内容设置好drivers和main入口函数的参数，--stage 0  --track oralM --steps 8000 --port 3001 --host localhost其中，各参数的含义，可见于代码文件的注释。配置如下图5所示。当第一次运行该执行文件的时候，不能直接点击上方工具栏的绿色三角形图标Run。我们选择将要执行的文件，在选中该文件界面的时候右击，然后选择“Run ‘snakeoil’”即可。若配置及程序正常的话，即可以看到如下图6所示的测试界面。图6中1处显示的是TORCS服务器的运行界面，在选中窗口1的时候，按F1键可以查看功能的帮助信息；图6中2处可以显示程序中的输出，比如Python中print函数的输出。代码分析RL框架的NFQ代码分析在强化学习（RL）任务中，主要包含智能体（Agent）和其生存的环境（Environment）两部分，如下图7所示，本project中的代码也是按照智能体和环境两部分进行组织的。本实验代码框架的实现参考了Pybrain库中的NFQ例子，对神经拟合网络Q-迭代（NFQ）代码的分析有助于对本project代码的理解，下图8对其中doEpisodes()和learn()两个核心函数进行解析。其清晰的图也可以在该网页处博文找到。其基本思想是：第一步，agent与环境进行交互，基于目前的策略进行利用和对未知策略的探索，即体现在doEpisodes()函数中；第二步，agent根据上一步得到的历史实验数据，进行学习，即learn()函数。不断循环，直到满足我们预设的任务要求再退出强化学习过程。比如，在倒立摆任务中，连续50次的摆动角度小于0.1度可以认为完成任务；或者预设学习多少次再退出程序。图8. NFQ中doEpisodes()和learn()函数的流程图解析本实验框架本project主要包含snakeoil.py, torcstask.py, torcsagent.py, cluster_dqfe.py（my_nfq）四个模块文件，以及statusanalysis.py对状态的分析和预处理pretrainAgent.py模块，共6处基本代码。其他文件是为了功能代码的组织而写。如前面NFQ算法的介绍，强化学习的学习过程可以分为两个步骤，不断往复：1.	跟环境交互进行策略的探索和利用；2.	基于自己探索的历史数据进行学习。本实验代码，根据该两部分，可以体现在下图8：下面对各个源文件的用途进行简要描述。 （如果开始不希望了解太多的过程，可以越过此部分，直接跳到实验示例部分）Snakeoil.py文件主要实现了与TORCS服务器的通信，将需要学习驾驶能力的Agent和其生存的Environment连接在一起；torcstask.py 文件主要是对环境的状态选取，奖励回报函数以及一些表征环境的功能变量进行定义，文件中已对重要代码进行了注释。一般变量的含义直接可以由名称体现出来；Torcsenvironment.py预留做torcs服务器的通信接口模块，目前并未实现内容，直接用的snakeoil的。仅仅在torcstask初始化中调用了，是一个空函数；各文件的演变过程及主要功能（所有的源文件建议从main函数开始读）：1.	基于snakeoil参加比赛的client.py源代码修改而来的myclient.py文件；2.	为训练程序的便利，基于myclient.py稍微修改，得到了train_main.py文件；3.	final_train_300_once.py文件是对一个建好的模型进行300次训练，将跑完全程的模型保存，写入./model/文件夹;并打印一些训练信息，将结果写入./data/文件夹中；是论文中300次试验训练用的文件。4.	train_for300_times.py是对final_train_300_once.py的功能性改进，有时的300次训练结果并不太理想，我们希望找到一次比较理想的结果，当第100次试验还没有找到一个解得时候，进行单次实验退出；是为了避免有时试验的不稳定性，而找到一些较好的模型。5.	test_with_multi_model_weight.py是对多个模型进行加权控制的简单尝试，目前并没有得到好的性能；是本人一个多模型投票表决动作想法的尝试，目前无解，可以忽略。6.	expReplay_size_analy.py 文件是是对经验池保留数据量的大小进行实验；如后文的实验2中第一部分解释。7.	cluster_size_analy.py文件是对聚类过程中数量的影响进行实验；如后文的实验2中第二部分解释。8.	50_test_for_cluster.py是对学习到的模型进行50次测试，并将一些测试结果写入到./data/目录下的文件里。是从上面cluster_size_analy.py文件抠出的一段代码，为方便测试用。9.	myactionnetwork.py 从Pybrain库中的actionnetwork文件修改而来，主要便于按照自己的要求建立网络结构，实际上是对actionnetwork的一个上层封装，预设了一些参量；10.	myNetworkreader.py与myactionnetwork.py一起工作可以在预训练的阶段，将预训练中的网络最后一层去掉，建立实验所需要的网络模型。实际上的做法是用myNetworkreader中的readFrom函数读取了除最后一层之外的网络模型，然后衔接上一个Q输出层；具体的调用可以参考pretrainAgent.py文件；以下两个独立文件是本人自主实现的主要代码：主要包括对训练数据样本的处理，然后进行策略模型的在线交互式训练。（关于NFQ、DQFE和DQFE-C代码的切换在torcsagent.py文件起始处用注释替换选择开关）如下图10,，即选择DQFE-C算法学习。11.	my_nfq.py文件的模板自于Pybrain库中的nfq算法，添加了对经验池的约束，是DQFE算法的一部分；12.	cluster_dqfe.py文件是基于my_nfq.py的框架做进一步改进，添加了对数据样本的聚类分析，是DQFE-C算法的一部分；实验示例1）	300次训练实验本实验主要是针对经验池回放数据进行约束处理的方法，进行实验及测试结果分析。其主要代码段在final_train_300_once.py文件。按照前文描述，设置好环境的参数后，启动TORCS，然后直接运行此文件即可。代码如下图11.其中，关于学习算法的选择如前文图10所示。	正常情况下就会在运行界面打印出如下图12的输出信息：分别显示了第几次实验得到的reward大小，以及单次训练所花费的时间。程序中，network训练，保存的流程关系，由下图13给出。2）	连续10次获取到策略模型实验A.	不同经验池大小的条件下，模型的学习与测试Train：为降低测试实验的复杂性，分别选取第10次完成学习任务（即连续3次通过预设道路的测试）的模型作为待测试网络模型，进行不同参数情况下的实验测试与分析对比。以下是对经验池大小设置不同参数的实验，具体请参见expReplay_size_analy.py代码，路径示下：C:\Users\xwei\PycharmProjects\mytorcs\qwells-10status-(0-1)0730-noQpretrain-noLP\qwells-10status-(0-1)\expReplay_size_analy.py主要有一段代码是在于：	对获取到的模型进行连续测试，当连续通过3次测试，才算本次模型的学习是有效的，即成功实现我们10次学习中的一次!!，该段入口代码如下图14.在代码中通过termination_flag，和test_ep 双重标志位实现，具体请参见代码，在下面语句处跳出while1循环，代表通过10次学习中的一次!!if termination_flag is not False:    termination_flag = False    each_work_time_consume.append(sum(learn_time_list))    break其他逻辑跟上面的300次试验相类似，直接运行该expReplay_size_analy.py文件即可，具体请参见代码。正常运行输出，示下图15. 显示信息的含义是：第几次学习任务（work），在当次任务里试验的次数，当次试验走了多少步，以及获取的reward信息，所有信息是自己定义输出的，即可以在源代码中自行修改。程序中，network训练，保存的流程关系，由下图16给出。Test：	训练完成后，所有通过全程道路的network会以.xml文件的形式被保存在./model/文件夹中。在expReplay_size_analy.py代码文件中，只要将LEARN_FLAG设置为False，即可进入test模式，如下图所示。直接run本文件，即可以对设置的network进行测试，并输出相关测试结果。B.	不同聚类数量对条件下，模型的学习与测试在torcsagent.py文件起始处打开注释from cluster_dqfe import NFQ如下图18,，即选择DQFE-C算法学习。关于采用Kmeans分类参数的实验cluster_size_analy.py，路径示下：C:\Users\xwei\PycharmProjects\mytorcs\qwells-10status-(0-1)0730-noQpretrain-noLP\qwells-10status-(0-1)\cluster_size_analy.py和本实验类似，设置分类的数量，下图19所示的是分为5、10类，两种情况的实验。具体可直接运行代码查看。重要区别是Kmeans分类模型，需要提前构建。在不同聚类数量train的过程中，会在analysis_learn()函数中读取聚类模型，该聚类模型是提前由cluster_dqfe.py文件生成的。保存在./model/kmeansModel目录下。	对于不同聚类参数下学习到模型的test，与上一部分不同经验池参数得到模型的测试一致，故不再赘述。#### Note：1.	在Python环境编码中，一般不要将运行代码直接写在源文件中，可以将每个模块文件中的执行代码按功能或者类的划分写成函数声明/类模块的形式。然后，将该文件的执行入口放在if __name__ == "__main__": 代码段处，并在该处放一些功能性测试语句， 当import该代码段的时候，并不会执行if __name__ == "__main__": 代码语句而提高效率；当需要对该源文件进行功能测试的时候，直接执行该文件即可。2.	关于赛道的定制，请参考手册build_your_trocs_track_in_20_minutes_v2.odt.3.	个人博客：http://towell.club/ 可能会更新部分内容，可交流讨论。##### 参考：
1.	Loiacono D, Cardamone L, Lanzi P L. Simulated car racing championship: Competition software manual [J]. arXiv preprint arXiv:1304.1672, 2013.2.	http://xed.ch/project/snakeoil/3.	Slides. David Silver. Lecture 1: Introduction to Reinforcement Learning.